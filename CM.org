#+TITLE : Prise de notes CM 4I500 ALGAV
#+PROPERTY: header-args :mkdirp yes
#+STARTUP: inlineimages

Antoine Genitrini (antoine.genitrini@sorbonne-universite.fr)
4I500

UE d'ouverture : fascicule de prérequis

* Informations pratiques

www.lip6.fr/Antoine.Genitrini

Amphi 45B

Evaluation :
0.2 Examen Réparti 1 + 0.2 Rendu Devoir de programmation + 0.6 Examen réparti 2

Devoir de programmation :
Suite du cours avec Binh Minh Bui Xuan

Cours d'ouverture inclus [[./CM0/cours0.pdf][ici]]

Cours d'algorithmique simple : 3I003 
Fanny Pascual

On aura le droit aux slides, plus aux notes sur les slides, plus une copie-double manuscrite.

** Prérequis

Il faut connaître :
- Notions de complexité, tri de liste (tri rapide, tri fusion)
- Complexité au pire cas
- Complexité en moyenne

On va surtout voir :
- Complexité en coût amorti

** Plan du cours

*** Structures arborescentes : files de priorité

Coût au pire cas n'est plus la mesure à suivre (ça l'était en licence)
Coût amorti != coût moyen

Coût de complexité en moyenne

*** Structures arborescentes pour la recherche

Recherche externe
Tries, arbres digitaux (texte)

*** Géométrie Algorithmique

Problème de collision d'objet.

*** Méthodes de Hachage

Fonction de hachage. Permet de récupérer une valeur en temps constant.

** Références

Froidevaux, Gaudel, Soria, _Types de données et algorithmique_
(Soria ancienne prof ici)

(Disponicle en ligne)
Beauquier, Berstel, Chrétienne, _Élements d'algorithmique_
Crochemore, Hancart, Lecroq, _Algorithmique du texte_


* Cours 0 : Cours d'ouverture, rappels

Ce cours présente un certain nombre d'algorithmes et de concepts d'analyse.

Les algorithmes seront écrits en C plutôt qu'en pseudo-langage.

** Le tri-insertion

Cet algorithme n'est pas du type "diviser pour régner", on parle donc d'algorithme naïf.

#+BEGIN_SRC c :tangle ./CM0/triinsertion.c
  #include <stdio.h>
  #include <stdlib.h>

  void inserer(int T[], int x, int e);
  void TriInserRec(int T[], int d, int f);
  void TriInserIter(int T[], int d, int f);
  void printTableau(int T[], int size);

  int main()
  {
	  int T[10] = {0};

	  // On définit la sentinelle : à défaut de l'infini, on met le plus petit nombre écrivable sur 4 octets
	  T[0] = -2147483648;

	  // On peuple le tableau
	  T[1] = 20;
	  T[2] = 19;
	  T[3] = 18;
	  T[4] = 17;
	  T[5] = 2;
	  T[6] = 14;
	  T[7] = 9;
	  T[8] = 3;
	  T[9] = 1;

	  printTableau(T, 10);

	  /* TriInserRec(T, 1, 10); */
	  TriInserIter(T, 1, 10);

	  printTableau(T, 10);

	  return 0;
  }

  void inserer(int T[], int x, int e)
  {
	  int k = e;

	  while (T[k] > x) {
		  T[k+1] = T[k];
		  k--;
	  }

	  T[k+1] = x;
  }

  // On a le choix entre la version récursive et itérative. On se permet d'implémenter les deux
  // Même si on ne se permettra pas d'utiliser la récursive

  void TriInserRec(int T[], int d, int f)
  {
	  if (d < f) {
		  TriInserRec(T, d, f - 1);
		  inserer(T, T[f], f - 1);
	  }
  }

  void TriInserIter(int T[], int d, int f)
  {
	  for (int i = d + 1; i < f; ++i) {
		  inserer(T, T[i], i - 1);
	  }
  }

  void printTableau(int T[], int size)
  {
	  for (int i = 0; i < size; ++i) {
		  printf("%d ", T[i]);
	  }

	  printf("\n");
  }
#+END_SRC

On part du principe qu'une partie de la liste est déjà triée de 0 à e, sans perte de généralité.

On prend l'élément d'indice e+1, et on le met en place en comparant de manière successive à tous les éléments à sa gauche.

Et on recommence jusqu'à arriver à la fin : e = size - 1.

*** Analyse de l'algorithme

**** Analyse en terminaison

On a bien un algorithme qui se termine : La boucle while de la fonction inserer a un nombre fini d'itérations (l'incrémentation est vers le bas, la barrière est "en bas").

La condition d'arrêt de la fonction TriInserRec finit toujours par être remplie : la variable f est décrémentée, la condition d'arrêt est de la forme f > qqch.

La boucle for de la fonction TriInserIter s'arrête forcément : la condition d'arrêt est de la forme i < qqch, et i est incrémentée.

**** Analyse en validité

A la fin d'une invocation de la fonction inserer, on a e + 1 éléments triés (si on partait du principe qu'on en avait e avant). A la fin de l'algorithme, e + 1 égale la taille du tableau, ce qui une autre manière de dire que le tableau est totalement trié.

**** Analyse en complexité

La fonction inserer fait au pire e + 1 comparaisons. e étant itéré de 0 à n-1 (n la taille du tableau), on a le nombre total de comparaisons donné par :

$\sum_{i=2}^{n}i$

Ce qui donne : $\frac{n(n+1)}{2} - 1$

L'algorithme du tri-insertion est donc au pire quadratique.

** Tri rapide

Cet algorithme est du type "diviser pour régner" : on se propose de découper un problème en problèmes plus petits, de les résoudre puis de les combiner.

*** Comportement asymptotique

On doit donner un certain nombres de concepts pour pouvoir correctement caractériser le comportement asymptotique d'un algorithme.

#+BEGIN_DEFINITION
Soit $\mathcal{F}_{\mathcal{N}}$ l'ensemble des fonctions de $\mathbb{N}$ dans $\mathbb{R}^{+}$.

Soient $f$ et $g$ deux fonctions de $\mathcal{F}_{\mathcal{N}}$.

On dit que $f$ est dominée par $g$ au voisinage de l'infini (ou encore que $g$ est une borne asymptotique supérieure de $f$) sssi :

$\exists c, n_0 > 0$ tels que $\forall n > n_0$, on a $f(n) < cg(n)$.

On peut noter $f = O(g)$ ou encore $g = \Omega(f)$, ces deux notations sont équivalentes.
#+END_DEFINITION

#+BEGIN_DEFINITION
On dira que $f$ et $g$ sont semblables au voisinage de l'infini (ou encore que $g$ est une borne asymptotique approchée de $f$) sssi $f$ est dominée par $g$ et $g$ est dominée par $f$. On notera :

$f = \Theta(g)$ (et donc aussi $g = \Theta(f)$)
#+END_DEFINITION

#+BEGIN_DEFINITION
On a aussi la notation suivante :

$\lim_{n \to +\infty} f(n)/g(n) = 0$ se note $f = o(g)$
#+END_DEFINITION

Le but de ces concepts est de pouvoir ramener la fonction de complexité asymptotique vers une fonction connue et écrivable, genre n, log(n), n^2, etc...

*** Retour au tri rapide

#+BEGIN_SRC c :tangle ./CM0/trirapide.c
  #include <stdio.h>
  #include <stdlib.h>


  void swap(int *op1, int *op2);
  int rearrangement(int T[], int p, int r);
  void quicksort(int T[], int p, int r);
  void printTableau(int T[], int size);

  int main()
  {
	  int T[10] = {0};

	  // On définit la sentinelle : à défaut de l'infini, on met le plus petit nombre écrivable sur 4 octets
	  T[0] = -2147483648;

	  // On peuple le tableau
	  T[1] = 20;
	  T[2] = 19;
	  T[3] = 18;
	  T[4] = 17;
	  T[5] = 2;
	  T[6] = 14;
	  T[7] = 9;
	  T[8] = 3;
	  T[9] = 1;

	  printTableau(T, 10);

	  quicksort(T, 0, 9);

	  printTableau(T, 10);

	  return 0;
  }

  void swap(int *op1, int *op2)
  {
	  int temp = *op1;
	  ,*op1 = *op2;
	  ,*op2 = temp;
  }

  int rearrangement(int T[], int p, int r)
  {
	  int v = T[r];
	  int i = p;

	  for (int j = p; j < r; ++j) {
		  if (T[j] <= v) {
			  swap(T + i, T + j);
			  ++i;
		  }
	  }

	  swap(T + i, T + r);

	  return i;
  }

  void quicksort(int T[], int p, int r)
  {
	  if (p < r) {
		  int q = rearrangement(T, p, r);
		  quicksort(T, p, q - 1);
		  quicksort(T, q + 1, r);
	  }
  }

  void printTableau(int T[], int size)
  {
	  for (int i = 0; i < size; ++i) {
		  printf("%d ", T[i]);
	  }

	  printf("\n");
  }

#+END_SRC


*** Preuve de terminaison




* Cours 1 : 17/09/2019 et 24/09/2019

** Chapitre 0

*** Notion de complexité

#+BEGIN_DEFINITION
Soit n la taille de l'entrée, et k une constante.

P : Se dit des problèmes qui se calculent en temps polynomial O(n^k)
EXP : Se calculent en temps exponentiel O(2^n)
NP : intermédiaire (Est-ce que les problèmes intermédiaires sont des problèmes P ou des problèmes NP-difficiles)
#+END_DEFINITION

On ne parlera pas des problèmes exponentiels.

Exemples de problèmes polynômiaux : tri, recherche, géométrie, texte, arithmétique.

*** Analyse d'algorithmes

Il faut définir une notion de taille (pas univoque, on peut en définir plusieurs).

Pour donner une complexité (nlog(n)), il faut donner aussi l'opération effectuée (permutation, etc...)

L'opération fondamentale doit être explicitée pour pouvoir permettre les comparaisons.

Plusieurs choses peuvent être comparées :
- Dans le meilleur des cas : min{ T_A(e) ; e \in E_n}
- Dans le pire des cas : max{ T_A(e) ; e \in E_n}
- En moyenne : 1 / (|E_n|) * \sum_{e \in E_n} T_A(e)

#+BEGIN_QUOTE
On pourrait à la limite donner une distribution de probabilité (et pas seulement un moment) d'un algorithme.
#+END_QUOTE

On introduit une nouvelle notion : complexité amortie, définie comme le coût d'une suite d'opération (donc moyenne des coûts).

*** Notions de mathématiques

On définit les trois notions principales :

#+BEGIN_DEFINITION
Soit $\mathcal{F}_{\mathcal{N}}$ l'ensemble des fonctions de $\mathbb{N}$ dans $\mathbb{R}^{+}$.

Soient $f$ et $g$ deux fonctions de $\mathcal{F}_{\mathcal{N}}$.

On dit que $f$ est dominée par $g$ au voisinage de l'infini (ou encore que $g$ est une borne asymptotique supérieure de $f$) sssi :

$\exists c, n_0 > 0$ tels que $\forall n > n_0$, on a $f(n) < cg(n)$.

On peut noter $f = O(g)$ ou encore $g = \Omega(f)$, ces deux notations sont équivalentes.
#+END_DEFINITION

#+BEGIN_DEFINITION
On dira que $f$ et $g$ sont semblables au voisinage de l'infini (ou encore que $g$ est une borne asymptotique approchée de $f$) sssi $f$ est dominée par $g$ et $g$ est dominée par $f$. On notera :

$f = \Theta(g)$ (et donc aussi $g = \Theta(f)$)
#+END_DEFINITION

#+BEGIN_DEFINITION
On a aussi la notation suivante :

$\lim_{n \to +\infty} f(n)/g(n) = 0$ se note $f = o(g)$

De fait, $f = o(g)$ implique $f = O(g)$

$f ~ g$ signifie $f - g = o(g)$ (équivalence)
#+END_DEFINITION

[[./CM1/ordredegrandeur.jpg][Comparaison d'ordres de grandeur asymptotique]]


** Chapitre 1 : Files de priorité

Complexité amortie : On est au plus proche de ce qui se passe en pratique.

Interclassement de liste : linéaire en la somme des tailles des deux listes.

*** Opérations de files de priorité

Ensemble d'éléments, chaque d'élément identifié par une clé, on veut trouver le minimum des clés. (typiquement une valeur de priorité pour un ordonnanceur)

Il faut un ordre total : on doit pouvoir comparer toujours deux éléments : on doit pouvoir dire cet élément-ci est plus petit/égal/plus grand que celui-là.


Opérations :

- On veut pouvoir ajouter un élément
- Supprimer l'élément de plus petite clé
- Construire une file avec n éléments reçus à la volée
- Union de plusieurs files de priorités
- Modification d'une clé

*** Tas

Un tas minimum : [insérer image]

#+ATTR_ORG: :width 600
[[./CM1/tasminimum.jpg][Tas minimum]]

Ensemble de valeurs distinctes deux à deux sous la forme d'un arbre. Contrainte : si on part de la racine vers les feuilles, tous les chemins possibles sont des suites strictement croissantes.

Trier un tas minimum est non-trivial (pas en temps linéaire).

On peut le construire en temps n, *donc* on le trie au moins en temps nlog(n)

*** Représentations des données et efficacité

#+ATTR_ORG: :width 600
[[./CM1/representationefficacite.jpg][Représentation et efficacité]]

[expliquer algorithme du tas, insertion]

*** Exemples

- Tri par tas (au lieu d'une liste) (heapsort)
- Sur les graphes (plus court chemin : Dijkstra ou A*), (plus court chemin entre tous les couples de sommets : Johnson), (arbre couvrant minimal : Prim)
- Interclassement de listes triées
- Compression de Huffmann


** Arbre binomial

Un arbre binomial est un graphe avec une racine et des sous-branches qui sont aussi des arbres binomiaux.

N'existent que si la taille est une puissance de 2.

#+BEGIN_DEFINITION
Définition par récurrence :

- B_0 est l'arbre de taille 2^0, un seul noeud

- Étant donné 2 arbres binomiaux B_k, on obtient B_{k+1} en faisant de l'un des B_k le premier fils (à gauche donc) à la racine de l'autre B_k.
#+END_DEFINITION

#+BEGIN_DEFINITION
Définition par induction forte :

- B_0 est l'arbre de taille 2^0, un seul noeud

- Pour construire B_k, je construis une nouvelle racine et je place tous les B_{k-1}, B_{k-2}, ..., B_0 de gauche à droite.
#+END_DEFINITION

Structure dite plane : les fils sont ordonnés de gauche à droite.

#+BEGIN_THEOREM
Un certain nombre de résultats intéressants :

Pour k >= 0

- B_k a 2^k noeuds (par construction)
- B_k a 2^k - 1 arêtes (suit de la précédente)
- B_k a hauteur k (démontrable par récurrence) (hauteur = nombre d'arêtes à traverser depuis la racine vers le fils le plus à gauche)
- Le degré à la racine (arité : nombre de fils) est k.
- Le nombre de noeuds à profondeur i est i parmi k.
- La forêt reliée à la racine de B_k est < B_{k-1}, B_{k-2}, ..., B_1, B_0 > (par construction)
#+END_THEOREM

#+BEGIN_PROOF
- Le premier résultat est évident par construction.
- Le deuxième suit du précédent trivialement : tous les noeuds ont exactement une arête entrante (qui vient du haut), sauf le noeud racine.
- Le troisième résultat se démontre par récurrence faible :

Initialisation (à 1) : évident.
Récurrence : Supposons la chose vraie à n un rang fixe. La construction de B_{n+1} supposant de placer B_n, B_{n-1}, etc... sous une nouvelle racine, dans l'ordre donné, de gauche à droite. Par définition de la hauteur, le chemin à parcourir est l'arête du nouvel arbre jusqu'à son fils immédiat le plus à gauche (1 traversée), puis de ce fils (ancienne racine de l'arbre B_n) jusqu'à son descendant final le plus à gauche (n traversées, par hypothèse de récurrence). On a donc bien n+1 traversées à faire pour arriver au descendant final le plus à gauche. Par application du principe de récurrence, on a B_k a hauteur k. CQFD

- Le quatrième résultat suit de la définition par induction forte plus haut, et suit directement du sixième résultat (qu'on suppose lui vrai par construction) : si la forêt reliée à la racine de B_k est <B_{k-1}, B_{k-2}, ...., B_{1}, B_{0}>

- Le cinquième résultat est le moins trivial à démontrer :

Par construction (en prenant la définition par induction forte), le nombre de noeuds à profondeur i de B_{k+1} est égal au nombre de noeuds à profondeur i de B_k (l'arbre B_k qu'on a fait le père de l'autre) plus le nombre de noeuds à profondeur i-1 de B_k (l'arbre B_k qu'on a fait le fils le plus à gauche de l'autre).

Si on donne n_{k,i} le nombre de noeuds de profondeur i de l'arbre B_k, alors la traduction formelle de la précédente intuition est donnée par :
$n_{k + 1,i} = n_{k,i} + n_{k,i-1}$

A partir de ça, on peut démontrer ce résultat par récurrence.
Initialisation (à 1) : ${1\choose 0} = {1\choose 1} = 1$. On a bien un noeud de profondeur 0 et un de profondeur 1.
On suppose l'égalité vérifiée au rang n, soit $n_{k,i} = {k\choose i}$. On doit montrer $n_{k+1,i} = {k+1\choose i}$, ce qui revient à montrer, par application de la formule de Pascal $n_{k+1,i} = {k\choose i} + {k\choose i-1}$.

Or, par application de la précédente intuition, on voit immédiatement le résultat à démontrer.
#+END_PROOF

*** Aparté

#+BEGIN_DEFINITION
Induction faible vs induction forte

Induction faible : je suppose P_k vraie
Induction forte : je suppose P_k, P_{k-1}, ..., P_0 vraies
#+END_DEFINITION


** File binomiale

#+BEGIN_DEFINITION
Un tournoi binomial (ou tas binomial) est un arbre binomial étiqueté croissant (croissance stricte sur tout chemin de la racine aux feuilles).

En plus, on veut que toutes les étiquettes soient distinctes.
#+END_DEFINITION

#+BEGIN_DEFINITION
Une file binomiale est une suite de tournois binomiaux de tailles strictement décroissantes.
#+END_DEFINITION

#+BEGIN_THEOREM
Corollaire de cette définition :

On peut donner une file binomiale (et une seule !) pour écrire n'importe quel nombre entier d'étiquettes. 

Quand on dit une seule, c'est la structure qui est unique, pas la manière d'étiqueter la structure.
#+END_THEOREM

#+BEGIN_THEOREM
Extension du théorème démontré en archi :

n = \alpha_0 2^0 + \alpha_1 2^1 + ... + \alpha_r 2^r

*Avec r = floor(log_2(n))*

log_2 est le logarithme binaire, soit la puissance à laquelle le nombre 2 doit être monté pour obtenir un nombre n.
#+END_THEOREM

#+BEGIN_THEOREM
Corollaire

Au plus, on a log(n) éléments dans la file. On sait que le minimum de toutes les étiquettes est à la racine d'un des éléments de la file. Donc la complexité de trouver le min dans une file binomiale est au plus d'ordre log(n).
#+END_THEOREM

#+BEGIN_THEOREM
Corollaire

Si n = 2^k, FB_n est la suite réduite à un unique tournoi binomial, le tournoi binomial TB_k.

Sinon la file binomiale est une suite de tournois correspondants aux bits égaux à 1 dans la représentation binaire de n.
#+END_THEOREM

#+BEGIN_DEFINITION
Poids de Hamming :

\nu(n) = \sum_i b_i

avec les b_i les bits à 1 dans la représentation binaire de n. Correspond au nombre de tournois qu'il faut pour écrire n.
#+END_DEFINITION

#+BEGIN_THEOREM
Propriétés de FB_n

- FB_n a n noeuds
- FB_n a (n - \nu(n)) arêtes.
- Le plus grand arbre binomial est B_{floor(log(n))}, le premier arbre de la file binomiale. De hauteur floor(log(n)) et nombre de noeuds 2^{floor(log(n))}.
- Le nombre d'arbres de la file est donné par \nu(n).
- Le minimum de la file est à la racine de l'un des arbres
#+END_THEOREM

#+BEGIN_PROOF
Le premier résultat est la définition.

Le deuxième résultat vient de ce que le nombre d'arêtes correspond au nombre de noeuds, moins le nombre de tournois (extension de la preuve plus haut par arête entrante, à \nu(n) plutôt que 1).

Le troisième résultat est vrai par construction.

Le quatrième est donné par la définition du poids de Hamming.

Le cinquième suit des conditions de stricte croissance et stricte distinction des étiquettes de chacun des tournois de la file.
#+END_PROOF

*** Union de files binomiales

On suppose que toutes les clés sont distinctes dans les N files à unir.

- Cas 1, union de 2 tournois (TB_l, TB_k) de tailles différentes :

D'après notre supposition, le cardinal de l'ensemble des clés des 2 tournois est de 2^l + 2^k.

On peut simplement donner F = < TB_l, TB_k >, qui est une file binomiale valable.

- Cas 2, union de 2 tournois de même taille.

On peut faire une file binomiale : F = TB_{k+1} qui pourra contenir toutes les étiquettes des deux tournois en entrée. Cette file binomiale doit respecter la propriété selon laquelle la racine est le plus petit élément du tournoi : on prend en fils de l'autre celui qui a la plus grande racine.

- Union de 2 files binomiales correspond à une addition binaire.

<TB_2, TB_0> U <TB'_2, TB'_1, TB'_0> = <TB''_3, TB''_2>

**** Aparté : l'addition binaire

En fait, c'est super facile de faire une addition en binaire : on peut se contenter de faire du bit par bit.

1 + 0 = 0 + 1 = 1
0 + 0 = 0
1 + 1 = +1 au bit de gauche (au bit de poids plus fort) (et 0 au bit courant)

**** Primitives

Ces primitives des pages 22 et 23 sont utilisables en examen : si on les appelle dans du pseudo-code, le correcteur (notre compilateur humain) saura ce que ça signifie.

On a droit à :

- EstVide(T) : Renvoie vrai si le tournoi T est vide
- Degre(T) : Renvoie le degré (un entier) de la racine du tournoi
- Union2Tid(T) : Renvoie l'union de deux tournois (un tournoi) de même taille
- Decapite(T) : Renvoie la file binomiale (suite de tournois) obtenue en supprimant la racine du tournoi T_k
- File(T) : Renvoie la file binomiale réduite au tournoi

- EstVide(F) : Renvoie vrai si la file F est vide
- MinDeb(F) : Renvoie le tournoi de degré minimal de la file F
- Reste(F) : Renvoie la file privee de son tournoi de degre minimal
- AjoutMin(T, F) : Renvoie la file obtenue en ajoutant le tournoi T comme tournoi de degré inférieur de la file F (ne fonctionne que si T est effectivement de degré plus petit que le degré minimal de la file passée en entrée)

Ces primitives peuvent aussi servir à définir des algorithmes de plus haut niveau. Si on a le temps, on pourrait implémenter ces primitives en C.

**** Analyse de complexité

On ne prouve pas la correction de l'algorithme (aussi vrai, ni plus ni moins, que la somme binaire).

La complexité de l'union de FB_n et FB_m est en O(log_2(n+m))

Critère de complexité : nombre de comparaisons entre clés (la création de pointeurs, copies de données ne comptent pas).

Idée principale : L'union de deux tournois de même taille nécessite 1 comparaison entre clés et ajoute une arête dans le file résultat. (l'union de deux tournois de taille différente est triviale : tournoi+grand, tournoi-grand, et ne nécessite pas de création d'arête ni de comparaion)

Conséquence : Le nombre de comparaisons égale le nombre d'arêtes de la file union moins le nombre d'arêtes des files de départ (en gros, les arêtes créées).

#+BEGIN_PROOF
Si on pose cette idée de manière formelle, ça donne :

$n + m - \nu(n+m) - (n - \nu(n)) - (m - \nu(m))$

Ce qui va se simplifier en :
$\nu(n) + \nu(m) - \nu(n+m)$

On a \nu(n) toujours inférieur à ceiling(log_2(n)).

On peut donc poser la quantité plus haut comme inférieure largement à :

$2\lfloor log_2(n + m) \rfloor + 2$

Cette borne supérieure est un grand O de log_2(n + m)
#+END_PROOF

*** Ajout d'un élément à une file

La meilleure manière, c'est de créer une file binomiale contenant seulement l'élément à ajouter, puis je fais l'union.

La complexité est donc donnée par \nu(n) + 1 - \nu(n + 1).

La complexité est entre 0 et \nu(n) : on a au max \nu(n) comparaisons.

*** Construction

La meilleure manière, c'est l'ajout de chacun des éléments un à un.

La complexité de la construction d'une file binomiale est donc donnée par la complexité de l'adjonction successive de ses n éléments.

Soit ici :

$\sum_{i=1}^{n-1} [\nu(i) + i - \nu(1+i)]$

C'est une somme téléscopique. Après simplification, la quantité totale de comparaisons est donnée par n - \nu(n)

Coût de la construction est le coût de la commutation de bits. [à clarifier avec Genetrini : Qu'est-ce que la commutation de bits ?]

**** Aparté : le coût amorti

#+BEGIN_DEFINITION
Dans une série d'opérations, on peut définir le coût amorti d'une opération comme la division du coût total par le nombre des opérations.

La définition plus générale, c'est le coût moyen d'une opération au pire cas. La manière de la définir plus haut est le résultat de la méthode dite par agrégat.

On regarde la pire configuration en totalité :

La somme des opérations de la pire configuration \neq n fois la pire opération.

Correspond au coût moyen d'une opération dans le pire cas, quelle que soit la suite d'opérations.
#+END_DEFINITION

*** Suppression du minimum

On sait par construction que le minimum de la file est à la racine d'un des tournois qui la composent.

La recherche prend donc \nu(n) - 1 comparaisons (le nombre de tournois - 1). Ce qui est un grand O de log_2(n).

La suppression consiste en supprimer la racine du tournoi qui porte la racine minimale : ça créé une deuxième file binomiale constituée des fils orphelins de la racine. On fait ensuite l'union de cette nouvelle file avec la file de départ privée du tournoi décapité.

Il reste, entre ces deux files, n - 1 éléments.

La recherche est toujours de complexité log_2(n). L'union des deux files sera aussi de complexité log_2(n) dans le pire des cas (cas particulier de la complexité général des unions de files binomiales).

*** Diminution d'une clé (ou étiquette)

On suppose un accès direct au noeud dont il faut diminuer la clé.

La meilleure manière, c'est de modifier la clé, puis échanger le noeud avec son père de manière successive jusqu'à ce que l'hypothèse de stricte croissance soit à nouveau respectée. Par construction, il y a forcément un moment où ça arrive. Par construction, on suppose aussi que la nouvelle valeur de la clé est distincte de toutes les autres (hypothèse de stricte distinction).

La complexité au pire cas, donné par le nombre maximum de comparaisons, est donc la hauteur de l'arbre, soit log_2(n) (ou log(n), à vérifier avec Genitrini)


* Cours 2 : 01/10/2019

On s'intéresse aux structures qui vont permettre de faire de la recherche, ici des structures arborescentes.

*** Problème de recherche

#+BEGIN_DEFINITION
Base de données

Ensemble d'éléments, chaque élément a une clé, on a un ordre total (deux clés différentes : une doit être plus petite que l'autre), on a un accès au bit des clés, les calculs se font sur les clés.

Sur lequels on est amené à faire les opérations suivantes :
- Recherche
- Ajouter
- Supprimer
- Construction
- Recherches partielles

Certaines structures sont plus ou moins bien adaptées aux différentes opérations.
#+END_DEFINITION

*** Efficacités par structure

On a l'efficacité en temps (nombre d'opérations) (plus important), et l'efficacité en mémoire (moins important).

[[./CM2/efficacite.jpg][Efficacité comparée]]

** Arbres binaires de recherche (ABR)

*** Arbres binaires

#+BEGIN_DEFINITION
Un arbre binaire est :

- soit vide
- soit constitué d'un noeud racine, et de deux sous-arbres (un gauche et un droit) qui sont aussi des arbres binaires

Soit $\mathcal{B} = \emptyset + <\bullet ,\mathcal{B}, \mathcal{B} >$
#+END_DEFINITION

#+BEGIN_DEFINITION
Un parcours est donné par :
$\mathcal{B} \to$ Liste(sommets)
#+END_DEFINITION

#+BEGIN_EXAMPLE
On a plusieurs parcours fréquemment utilisés.

Le parcours préfixe :

PREF(B) = [visit(\bullet), PREF(G), PREF(D)]

Le parcours préfixe d'un ABR ne donne aucune information sur l'ordre des clés.
Chaque noeud n'est visité qu'une seule fois : coût O(n)

Le parcours infixe :

INF(B) = [INF(G), visit(\bullet), INF(D)]

Le parcours infixe donne a priori renvoie une liste triée des clés.
Coût aussi O(n).

Le parcours suffixe :

SUF(B) = [SUF(G), SUF(D), visit(\bullet)]

Chaque parcours envoie en ordre différent la liste.
#+END_EXAMPLE

#+BEGIN_DEFINITION
On peut compléter les noeuds vides par une feuille. L'arbre obtenu s'appelle le complété. L'arbre complété à n noeuds a en plus n + 1 feuilles
#+END_DEFINITION

*** ABR

#+BEGIN_DEFINITION
Un arbre binaire de recherche est un arbre binaire tel que :

En chaque noeud, l'étiquette est plus grande que toutes les étiquettes du sous-arbre gauche, et plus petite que toutes les étiquettes du sous-arbre droit.
#+END_DEFINITION

#+BEGIN_THEOREM
Propriété

Le parcours infixe d'un ABR donne la suite des étiquettes en ordre croissant.
#+END_THEOREM

#+BEGIN_PROOF
La preuve se fait par induction forte.
#+END_PROOF

**** Algorithmes de recherche, ajout et suppression

Peuvent se faire en parcourant seulement une seule branche (c'est pour ça qu'on s'en sert).
L'insertion se fait à un unique endroit, facile à déterminer.
La suppression d'un noeud interne (pas tout en bas) requiert un peu de travail parmi les sous-arbres.

**** Primitives

On a un certain nombre de primitives dont on aura le droit de se servir en examen, pour définir des algorithmes en pseudo-langages de plus haut niveau :
- ArbreVide : renvoie l'arbre vide
- ArbreBinaire(e,G,D) : renvoie l'arbre binaire formé de l'élément e et des sous-arbres G à gauche et D à droite
- EstArbreVide(A) : renvoie vrai sssi l'arbre est vide
- Racine(A) : Renvoie le contenu de la racine de A.
- SousArbreGauche(A) : Renvoie une copie du sous-arbre gauche
- SousArbreDroit(A) : Renvoie une copie du sous-arbre droit
- Pere(A) : Renvoie l'arbre dont A est un des sous-arbre immédiat (un des fils de la racine), ou l'arbre vide, si A n'est pas un sous-arbre.

**** ABR équilibrés

#+BEGIN_DEFINITION
Soit un ABR quelconque

La hauteur moyenne est en O(log(n)), mais au pire on a un peigne, ce qui implique une hauteur O(n).
#+END_DEFINITION

On peut avoir un ABR très déséquilibré (ici celui de droite, appelé peigne) :

[[./CM2/ABR.png][ABR]]

Dans le pire des cas, on a besoin de n comparaisons pour trouver l'entier n.

On a des manières de forcer le rééquilibrage par rotation si l'amplitude
(On ne gagne pas en moyenne, mais on diminue la probabilité du pire cas)

** Arbres équilibrés

#+BEGIN_DEFINITION
Un arbre équilibré est un arbre dont la hauteur est toujours en O(log(n)).
#+END_DEFINITION

#+BEGIN_THEOREM
Soit la liste des entiers de 1 à n. Le nombre total de mélanges est donné par n!.

Etant donné un mélange, soit un ordre d'arrivée, on a un unique arbre.

Ces trois propositions disent à peu près la même chose vague :
- En fait, très peu de mélanges donnent un arbre déséquilibré.
- On a moins d'arbres que de mélanges. Les arbres équilibrés ont une probabilité forte d'advenir.
- Plus l'arbre est équilibré, plus il y a de mélanges qui débouchent sur l'arbre.
#+END_THEOREM

On va avoir besoin d'algorithmes sophistiqués pour préserver l'équilibre.

*** Processus de rotation

On fait l'insertion, on a dépassé le seuil (par exemple, j'avais des noeuds vides à un étage qui n'est pas le dernier)
On remonte au père du noeud vide à remplir en partant de l'étiquette qu'on vient de rajouter.
On fait une rotation vers le noeud vide (donc vers la gauche ou la droite) en partant de son père.

En général, il est trop coûteux d'obtenir un ABR parfait.

On se permet alors de relâcher les contraintes :
Sur la hauteur, donne la famille des arbres AVL.
Sur le remplissage des noeuds d'un étage (la largeur), donne la famille des arbres B.

*** Exemples de structures

Structures typiques pour la RAM :
- AVL
- Arbre 2-3-4 (Red-black)

Structures typiques pour le disque (plus de données, base de données) :
- Arbre B
- Arbre auto-adaptatif

*** AVL (Adelson-Velsky, Landis)

#+BEGIN_DEFINITION
Un AVL est un ABR, mais en plus, en chaque noeud, la hauteur du sous-arbre gauche et celle du sous-arbre droit diffèrent au plus de 1.
#+END_DEFINITION

#+BEGIN_THEOREM
Soit h la hauteur d'un AVL avec n noeuds :

log_2(n+1) \leq h + 1 < 1.44 log_2(n)
#+END_THEOREM

#+BEGIN_PROOF
La preuve de la borne de droite est donné par la considération des pires arbres AVL, les arbres de Fibonacci :

F_0 = < \bullet , \emptyset , \emptyset >
F_1 = < \bullet , F_0 , \emptyset >
F_n = < \bullet , F_{n-1} , F_{n-2} >

C'est le pire arbre AVL possible dans le sens où il contient le moins d'étiquettes pour une hauteur donnée. (pas vraiment une démonstration, on a juste l'idée).
#+END_PROOF

*** Rotations

**** Rotation droite :

A = < q, < p, U, V >, W > \to RD(A) = < p, U, < q, V, W > >

Avec U le sous-arbre de hauteur +1.

[[./CM2/rotationdroite.jpg][Rotation droite]]

**** Rotation gauche :

A = < p, U, < q, V, W >, W > \to RG(A) = < q, < p, U, V >, W >

**** Rotation gauche-droite :

A = < q, < p, U, < r, V_{1}, V_{2} >, >, W > \to RGD(A) = < r, < p, U, V_{1} >, < q, V_{2}, W > >

[[./CM2/rotationgauchedroite.jpg][Rotation gauche-droite]]

Toutes ces rotations sont très peu coûteuses.

**** Rotation droite-gauche :

A = < q, W, < p, < r, V_{1}, V_{2} >, U > > \to RDG(A) = < r, < q, W, V_{1} >, < p, V_{2}, U > >

*** Primitives

On dispose d'un certain ensemble de primitives sur les AVL :
- Hauteur(A) : Renvoie la hauteur de l'arbre passé en argument.
- Les quatre fonctions de rotation mentionnées plus haut.
- Equilibrage(A) : Suppose que A est un arbre de recherche, que ses sous-arbres sont des AVL, dont les hauteurs diffèrent de 2 au plus. Renvoie l'arbre obtenu en rééquilibrant l'arbre initiale (Détermine laquelle des rotations à appliquer, et l'applique)
- AVL_Ajout(x, A) : Renvoie l'AVL résultant de l'ajout de x à A.


* Cours 3 : 08/10/2019

** Arbres équilibrés, suite

*** Arbre de recherche général

#+BEGIN_DEFINITION
Dans un arbre de recherche général :

- Chaque noeud contient un k-uplet (e_1 < e_2 < ... < e_k) d'éléments distincts et ordonnées.

Chaque noeud a k+1 sous-arbres A_1, ..., A_{k+1} tels que :
- Tous les éléments de A_1 sont < e_1
- Tous les éléments de A_i sont > e_{i-1} et < e_i pour i allant de 2 à k
- Tous les éléments de A_{k+1} sont > e_k
#+END_DEFINITION

Un cas particulier intéressant, particulièrement souvent utilisé :

#+BEGIN_DEFINITION
Arbres 2-3-4 :

Les noeuds internes contiennent des k-uplets de 1, 2 ou 3 éléments (respectivement appelés des 2-, 3- ou 4-noeuds, avec respectivement 2, 3 et 4 sous-arbres).

Toutes les feuilles sont au même niveau.
#+END_DEFINITION

#+BEGIN_THEOREM
h la hauteur d'un arbre 2-3-4 avec n éléments : h \in \Theta(log(n))
#+END_THEOREM

#+BEGIN_PROOF
Si un arbre n'a que des 2-noeuds (une seule valeur par noeud) :

h+1 = log_2(n+1)

Si un arbre n'a que des 4-noeuds (3 valeurs par noeuds) :

h+1 = log_4(n+1)
#+END_PROOF

#+BEGIN_THEOREM
Un ensemble de corollaires :

- Tous les sous-arbres d'un arbre 2-3-4 sont des arbres 2-3-4.
#+END_THEOREM

**** Primitives

On dispose d'un ensemble de primitives utiles, plus pratiques pour les algorithmes plus complexes :
- EstVide(A)
- Degre(A) : l'arité à la racine de A (ou l'arité de A si on considère A le noeud racine de l'arbre plutôt que l'arbre lui-même)
- Contenu(A) : la liste (ordonnée croissante) des éléments de la racine de A (ou la liste de A si on considère A le noeud racine de l'arbre plutôt que l'arbre lui-même)
- EstDans(x, L) : Est-ce que x est dans la liste L
- Elem_i(A) : Renvoie le ième élément de la racine de A (+\infty si i est trop grand : genre on a demandé Elem_3 d'un 2-noeud qui ne contient qu'un élément)
- SsA_i(A) : Renvoie le ième sous-arbre de la racine de A (l'arbre vide si i est trop grand : genre on a demandé SsA_3 d'un 2-noeud qui n'a que 2 sous-arbres)

A partir de ces primitives, on peut donner un algorithme de recherche en pseudo-langage.

**** Recherche d'un élément

On compare l'élément recherché aux éléments du noeud racine. Soit on le trouve, soit on sait au bout de 1-3 comparaisons dans quel sous-noeud on le doit rechercher. Et ainsi de suite.

On comprend aisément que la recherche est meilleure si on a des 4 noeuds (encore qu'on est quand même en O(log(n)) dans tous les cas) :

#+BEGIN_PROOF
Soit un arbre avec que des 2-noeuds : 1 comparaison par étage, soit log_2(n+1) comparaisons en tout.

Soit un arbre avec que des 4-noeuds : 3 comparaisons par étage dans le pire des cas, soit log_4(3n) comparaisons en tout (log_4(3n))

On sait qu'on se situe entre ces deux cas extrêmes : quoiqu'il en soit, on sait qu'on est en complexité O(log(n)) en le nombre de comparaisons.
#+END_PROOF

**** Ajout d'un élément

L'ajout est guidé par la recherche : on met l'élément où on l'aurait trouvé par la recherche. On s'arrête quand ?

Déjà, on ne s'arrête que si le noeud est plein ou si on est arrivé en bas.

Si celui-ci est plein, alors on doit éclater le noeud (version éclatement systématique en descente) :
- On garde un pointeur où on aurait dû mettre l'élément
- L'élément du mileu devient la racine, le plus petit devient le fils gauche, le plus grand devient le fils droit (même niveau donc).
- On met l'élément à sa place en recommençant à chercher au pointeur (qu'on a pris la peine de garder avant), dans l'endroit de plus bas niveau.

Si on a un éclatement sur une feuille du bas, on doit faire remonter la valeur centrale de l'élément à éclater dans un noeud plus haut (dont on sait qu'il n'est pas plein, puisqu'on vient de le traverser).

***** Aparté, exemple

On se propose de construire par ajout successif un arbre 2-3-4 avec les éléments, en utilisant l'algorithme d'éclatement systématique en descente  :

(4, 35, 10, 13, 3, 30, 15, 12, 7, 40, 20, 11, 6)

| 4 |

| 4, 35 |

| 4, 10, 35 |
Eclatement ! On garde le pointeur avant 35

|   | 10 |    |
| 4 |    | 35 |
On reprend la recherche à partir du pointeur

|   | 10 |       |
| 4 |    | 13,35 |

|     | 10 |       |
| 3,4 |    | 13,35 |

|     | 10 |          |
| 3,4 |    | 13,30,35 |
Eclatement ! On garde le pointeur avant 30

|     | 10,30 |    |
| 3,4 |    13 | 35 |
On reprend la recherche à partir du pointeur

|     | 10,30 |    |
| 3,4 | 13,15 | 35 |

|     | 10,30    |    |
| 3,4 | 12,13,15 | 35 |

|       | 10,30    |    |
| 3,4,7 | 12,13,15 | 35 |

|       | 10,30    |       |
| 3,4,7 | 12,13,15 | 35,40 |
Eclatement ! On garde le pointeur après 15

|       |    | 10,13,30 |       |
| 3,4,7 | 12 | 15       | 35,40 |
On reprend la recherche à partir du pointeur

|       |    | 10,13,30 |       |
| 3,4,7 | 12 | 15,20    | 35,40 |
Eclatement ! On garde le pointeur avant 13

|       |    |    | 13 |       |    |       |
|       | 10 |    |    |       | 30 |       |
| 3,4,7 |    | 12 |    | 15,20 |    | 35,40 |
On reprend la recherche à partir du pointeur

|       |    |       | 13 |       |    |       |
|       | 10 |       |    |       | 30 |       |
| 3,4,7 |    | 11,12 |    | 15,20 |    | 35,40 |
Eclatement ! On garde le pointeur avant 7

|   |      |       | 13 |       |    |       |
|   | 4,10 |       |    |       | 30 |       |
| 3 |    7 | 11,12 |    | 15,20 |    | 35,40 |
On reprend la recherche à partir du pointeur

|   |      |       | 13 |       |    |       |
|   | 4,10 |       |    |       | 30 |       |
| 3 | 6,7  | 11,12 |    | 15,20 |    | 35,40 |

***** Fin de l'aparté

#+BEGIN_THEOREM
On ne peut faire croître la hauteur de l'arbre qu'en éclatant la racine.
#+END_THEOREM

On a deux manières de déterminer quand faire un éclatement :
- Eclatement systématique en descente (présenté ici : on éclate dès qu'on rencontre un noeud plein)
- Eclatement en remontée (on éclate que si on rencontre un noeud plein tout au bout)

***** Avantages et inconvénients

En descente :
- Parcours uniquement de haut en bas (seulement besoin de pointeurs simples) (+)
- Transformation très locale (on éclate un noeud, et on remonte pas plus haut que le père : nécessite un unique pointeur supplémentaire) (+)
- Taux d'occupation plus faible (moins bon pour la recherche) (-)
- Hauteur plus grande (-)

En remontée :
- Parcours dans les deux sens (double pointeurs, empreinte mémoire) (-)
- Transformation en chaîne de proba non-nulle (même de proba 1 en n le nombre d'ajouts) (-)
- Taux d'occupation plus fort (+)
- Hauteur plus petite (+)

*** Arbres B

Structure ultra-majoritaire des bases de données.
Existe depuis 30 ans.

Implémenté en C++ dans la bibliothèque stxxl. Oracle et Google ont aussi des implémentations à eux.

L'idée est de limiter autant qu'il est possible les défauts de page, de manière à éviter les accès disques.

Pour cette raison, on veut blinder les noeuds de données, de manière à ce qu'un noeud occupe à peu près une page mémoire. Et savoir exactement quelle page mémoire aller chercher sur le disque si on a suffisemment peu de chance pour en avoir besoin.

#+BEGIN_DEFINITION
Arbre-B d'ordre m est un arbre de recherche

Arbre de recherche dont les noeuds contiennent k éléments, k entre m et 2m.
Sauf la racine qui contient entre 1 et 2m.

Toutes les feuilles sont situées au même niveau.
#+END_DEFINITION

#+BEGIN_THEOREM
Hauteur d'un B-arbre d'ordre m contenant n éléments

log_{2m+1}(n+1) \leq h + 1 \leq 1 + log_{m+1}((n+1)/2)
#+END_THEOREM

La hauteur diminue énormément en m. La hauteur doit être aussi petite que possible, car passer d'un noeud à un de ses fils consiste précisément en faire cette entrée/sortie disque tant redoutée.

On choisit m tel que le contenu de la racine doit pouvoir être stocké en RAM. Un noeud doit tenir dans une page.
Les autres noeuds seront stockés dans le disque.

1 éclatement implique écrire deux pages en disque (très cher)

#+BEGIN_THEOREM
Le nombre d'éclatement par clé dans la construction par adjonctions successives d'un B-arbre d'ordre m est compris entre 1/(2m) et 1/m.
#+END_THEOREM

#+BEGIN_PROOF
Deux cas de figure extrêmes après ajout successif de tous les n éléments :

- Soit tous les noeuds (y compris la racine) ont leur nombre maximal d'éléments, on a donc le nombre de noeuds donné par n/2m
- Soit tous les noeuds (y compris la racine) ont leur nombre minimal d'éléments, on a donc le nombre de noeuds donné par 1 + (n-1)/m

Le nombre total d'éclatements étant égal au nombre de noeuds final après ajout de tous les éléments, on a E le nombre d'éclatements \in [n/2m ; n/m]
#+END_PROOF

#+BEGIN_THEOREM
Résultats supplémentaires :

1 éclatement pour 1.38m adjonctions
Un B-arbre d'ordre m contenant n éléments aléatoires comporte 1.44n/m noeuds.
#+END_THEOREM

#+BEGIN_PROOF
Preuve : Bla bla Fibonacci bla bla bla, ou un truc du genre.
#+END_PROOF

*** Arbres auto-adaptatifs

#+BEGIN_DEFINITION
Un arbre auto-adaptatif est un arbre binaire de recherche tel qu'à chaque recherche, ajout, suppression, le dernier élément visité est remonté à la racine par une suite de rotations.
#+END_DEFINITION

Intuition : Quand je fais remonter qqch de très profond, l'arbre résultant est plutôt équilibré.

#+BEGIN_THEOREM
Fonction de potentiel d'un arbre T

Poids de x \in T : w(x) = nb noeuds du ss-arbre de racine x (x inclus).

La fonction de potentiel T est donnée par :

\Phi(T) = \sum_{x \in T} r(x), avec r(x) = log_2(w(x)), rang de x.
#+END_THEOREM

Le coût au pire cas est en O(n), cher. Mais en fait, un opération coûteuse (ramener un truc de tout au fond) accélère grandement les opérations futures, simplement parce qu'on rééquilibre énormément l'arbre.

L'analyse par potentiel permet de monter que pour toute suite de m opérations, le coût est en O(m log(n)).

** Tries

Structures dont les clés sont des chaînes de caractère. Adaptation des structures précédentes.

Les clés sont définissables caractère par caractère : on peut la voir bit par bit.

L'idée est de partager les préfixes de deux mots par exemple.

Opérations souhaitées :
- On veut effectuer la recherche, insérer, supprimer des clés.
- On veut pouvoir faire la liste en ordre alphanumérique.
- Recherche partiellement spécifiées (regex)
- Recherche du plus long préfixe dans S d'un mot donné.

*** Primitives

On a un ensemble de primitives utiles, qui vont permettre de définir des algorithmes de plus haut niveau :
- prem(cle) : renvoie le premier caractère de la clé
- reste(cle) : Renvoie la clé, sans son premier caractère
- car(cle, i) : Renvoie le ième caractère de la clé
- lgueur(cle) : Renvoie le nombre de caractères de la clé

*** DST (Arbres digitaux)

#+BEGIN_DEFINITION
Un arbre binaire dont les noeuds contiennent les clés.

Le principe de la recherche est le même que pour les ABR, mais l'aiguillage se fait non par comparaison entre clés, mais selon les bits de la clé cherchée (bit de poids fort au premier niveau, etc...)
#+END_DEFINITION

La structure de l'arbre va dépendre de l'ordre d'insertion.

On doit décaler l'espace des caractères à écrire au "centre" de l'espace des caractères pouvant être écrits (en terme de nombre de bits) ou alors s'assurer ce que les éléments encodés sont uniformément répartis sur l'espace d'écriture.

Idéalement, on doit commencer par insérer du milieu d'alphabet (le truc le plus proche de 0b10000000
soit la valeur à égale distance de 0b00000000 et de 0b11111111)

#+BEGIN_THEOREM
Le recherche ou l'ajout d'une clé dans un DST contenant n clés nécessite en moyenne log(n) comparaisons de clés, et au pire L (nombre maximum de bits d'une clé)
#+END_THEOREM

*** Arbres lexicographiques

#+BEGIN_DEFINITION
Un arbre lexicographique, ou un trie binaire :

Les noeuds internes servent juste d'aiguillage (pas d'information dedans).
Pour chaque noeud vide dont les bits depuis la racine sont donnés par w un vecteur de bits, le sous-arbre gauche contient toutes les clés commençant par (w, 0) et le sous-arbre droit contient toutes les clés commençant par (w, 1).
#+END_DEFINITION

Les clés sont en ordre croissant d'encodage.
La hauteur maximale est donnée par le nombre de bits d'encodage.

Notion de Patricia tries : on fusionne les noeuds qui n'ont qu'un fils avec le fils (on doit noter qqpart que l'arête qui va dans ce nouveau noeud est en fait plusieurs arêtes en une, donc plusieurs bits déterminés d'un coup)

*** R-Tries

#+BEGIN_DEFINITION
Un R-trie est un arbre dont tous les noeuds sont d'arité R et servent d'aiguillage. Chaque noeud a aussi une valeur, qui est non-vide s'il représente une clé.
#+END_DEFINITION

Recherche, ajout et suppression se fait par parcours de branche.

#+BEGIN_THEOREM
La structure ne dépend pas de l'ordre d'insertion.
#+END_THEOREM

Par exemple, un 26-trie peut avoir tous les mots de la langue française en une structure unique (pourvu qu'on n'écrive pas les accents).

#+BEGIN_THEOREM
Le recherche ou l'ajout d'une clé dans un R-trie contenant n clés nécessite en moyenne log(n) comparaisons de caractères, et au pire L (nombre maximum de bits d'une clé).
#+END_THEOREM

Le problème de cette structure vient de son empreinte mémoire. On a besoin de R pointeurs par noeud, la plupart d'entre eux vides. En plus, les langages naturels ne débouchent pas sur des 26-trie équilibrés : on a beaucoup plus de mots qui commencent par a que z.

*** Trie hybride

#+BEGIN_DEFINITION
Arbre ternaire :

Chaque noeud contient un caractère et une valeur. Chaque neoud a trois pointeurs : un lien Inf, Eq, Sup vers le sous-arbre dont le premier caractère est inférieur ou égal ou supérieur à son caractère.
#+END_DEFINITION


* Annexes

Support des cours :

[[./CM1/cours1.pdf][Cours 1]]
[[./CM2/cours2.pdf][Cours 2]]






