#+TITLE : Prise de notes CM 4I500 ALGAV
#+PROPERTY: header-args :mkdirp yes
#+STARTUP: inlineimages

Antoine Genitrini (antoine.genitrini@sorbonne-universite.fr)
4I500

UE d'ouverture : fascicule de prérequis

* Informations pratiques

www.lip6.fr/Antoine.Genitrini

Amphi 45B

Evaluation :
0.2 Examen Réparti 1 + 0.2 Rendu Devoir de programmation + 0.6 Examen réparti 2

Devoir de programmation :
Suite du cours avec Binh Minh Bui Xuan

Cours d'ouverture inclus [[./CM0/cours0.pdf][ici]]

Cours d'algorithmique simple : 3I003 
Fanny Pascual

On aura le droit aux slides, plus aux notes sur les slides, plus une copie-double manuscrite.

** Prérequis

Il faut connaître :
- Notions de complexité, tri de liste (tri rapide, tri fusion)
- Complexité au pire cas
- Complexité en moyenne

On va surtout voir :
- Complexité en coût amorti

** Plan du cours

*** Structures arborescentes : files de priorité

Coût au pire cas n'est plus la mesure à suivre (ça l'était en licence)
Coût amorti != coût moyen

Coût de complexité en moyenne

*** Structures arborescentes pour la recherche

Recherche externe
Tries, arbres digitaux (texte)

*** Géométrie Algorithmique

Problème de collision d'objet.

*** Méthodes de Hachage

Fonction de hachage. Permet de récupérer une valeur en temps constant.

** Références

Froidevaux, Gaudel, Soria, _Types de données et algorithmique_
(Soria ancienne prof ici)

(Disponicle en ligne)
Beauquier, Berstel, Chrétienne, _Élements d'algorithmique_
Crochemore, Hancart, Lecroq, _Algorithmique du texte_


* Cours 0 : Cours d'ouverture, rappels

Ce cours présente un certain nombre d'algorithmes et de concepts d'analyse.

Les algorithmes seront écrits en C plutôt qu'en pseudo-langage.

** Le tri-insertion

Cet algorithme n'est pas du type "diviser pour régner", on parle donc d'algorithme naïf.

#+BEGIN_SRC c :tangle ./CM0/triinsertion.c
  #include <stdio.h>
  #include <stdlib.h>

  void inserer(int T[], int x, int e);
  void TriInserRec(int T[], int d, int f);
  void TriInserIter(int T[], int d, int f);
  void printTableau(int T[], int size);

  int main()
  {
	  int T[10] = {0};

	  // On définit la sentinelle : à défaut de l'infini, on met le plus petit nombre écrivable sur 4 octets
	  T[0] = -2147483648;

	  // On peuple le tableau
	  T[1] = 20;
	  T[2] = 19;
	  T[3] = 18;
	  T[4] = 17;
	  T[5] = 2;
	  T[6] = 14;
	  T[7] = 9;
	  T[8] = 3;
	  T[9] = 1;

	  printTableau(T, 10);

	  /* TriInserRec(T, 1, 10); */
	  TriInserIter(T, 1, 10);

	  printTableau(T, 10);

	  return 0;
  }

  void inserer(int T[], int x, int e)
  {
	  int k = e;

	  while (T[k] > x) {
		  T[k+1] = T[k];
		  k--;
	  }

	  T[k+1] = x;
  }

  // On a le choix entre la version récursive et itérative. On se permet d'implémenter les deux
  // Même si on ne se permettra pas d'utiliser la récursive

  void TriInserRec(int T[], int d, int f)
  {
	  if (d < f) {
		  TriInserRec(T, d, f - 1);
		  inserer(T, T[f], f - 1);
	  }
  }

  void TriInserIter(int T[], int d, int f)
  {
	  for (int i = d + 1; i < f; ++i) {
		  inserer(T, T[i], i - 1);
	  }
  }

  void printTableau(int T[], int size)
  {
	  for (int i = 0; i < size; ++i) {
		  printf("%d ", T[i]);
	  }

	  printf("\n");
  }
#+END_SRC

On part du principe qu'une partie de la liste est déjà triée de 0 à e, sans perte de généralité.

On prend l'élément d'indice e+1, et on le met en place en comparant de manière successive à tous les éléments à sa gauche.

Et on recommence jusqu'à arriver à la fin : e = size - 1.

*** Analyse de l'algorithme

**** Analyse en terminaison

On a bien un algorithme qui se termine : La boucle while de la fonction inserer a un nombre fini d'itérations (l'incrémentation est vers le bas, la barrière est "en bas").

La condition d'arrêt de la fonction TriInserRec finit toujours par être remplie : la variable f est décrémentée, la condition d'arrêt est de la forme f > qqch.

La boucle for de la fonction TriInserIter s'arrête forcément : la condition d'arrêt est de la forme i < qqch, et i est incrémentée.

**** Analyse en validité

A la fin d'une invocation de la fonction inserer, on a e + 1 éléments triés (si on partait du principe qu'on en avait e avant). A la fin de l'algorithme, e + 1 égale la taille du tableau, ce qui une autre manière de dire que le tableau est totalement trié.

**** Analyse en complexité

La fonction inserer fait au pire e + 1 comparaisons. e étant itéré de 0 à n-1 (n la taille du tableau), on a le nombre total de comparaisons donné par :

$\sum_{i=2}^{n}i$

Ce qui donne : $\frac{n(n+1)}{2} - 1$

L'algorithme du tri-insertion est donc au pire quadratique.

** Tri rapide

Cet algorithme est du type "diviser pour régner" : on se propose de découper un problème en problèmes plus petits, de les résoudre puis de les combiner.

*** Comportement asymptotique

On doit donner un certain nombres de concepts pour pouvoir correctement caractériser le comportement asymptotique d'un algorithme.

#+BEGIN_DEFINITION
Soit $\mathcal{F}_{\mathcal{N}}$ l'ensemble des fonctions de $\mathbb{N}$ dans $\mathbb{R}^{+}$.

Soient $f$ et $g$ deux fonctions de $\mathcal{F}_{\mathcal{N}}$.

On dit que $f$ est dominée par $g$ au voisinage de l'infini (ou encore que $g$ est une borne asymptotique supérieure de $f$) sssi :

$\exists c, n_0 > 0$ tels que $\forall n > n_0$, on a $f(n) < cg(n)$.

On peut noter $f = O(g)$ ou encore $g = \Omega(f)$, ces deux notations sont équivalentes.
#+END_DEFINITION

#+BEGIN_DEFINITION
On dira que $f$ et $g$ sont semblables au voisinage de l'infini (ou encore que $g$ est une borne asymptotique approchée de $f$) sssi $f$ est dominée par $g$ et $g$ est dominée par $f$. On notera :

$f = \Theta(g)$ (et donc aussi $g = \Theta(f)$)
#+END_DEFINITION

#+BEGIN_DEFINITION
On a aussi la notation suivante :

$\lim_{n \to +\infty} f(n)/g(n) = 0$ se note $f = o(g)$
#+END_DEFINITION

Le but de ces concepts est de pouvoir ramener la fonction de complexité asymptotique vers une fonction connue et écrivable, genre n, log(n), n^2, etc...

*** Retour au tri rapide

#+BEGIN_SRC c :tangle ./CM0/trirapide.c
  #include <stdio.h>
  #include <stdlib.h>


  void swap(int *op1, int *op2);
  int rearrangement(int T[], int p, int r);
  void quicksort(int T[], int p, int r);
  void printTableau(int T[], int size);

  int main()
  {
	  int T[10] = {0};

	  // On définit la sentinelle : à défaut de l'infini, on met le plus petit nombre écrivable sur 4 octets
	  T[0] = -2147483648;

	  // On peuple le tableau
	  T[1] = 20;
	  T[2] = 19;
	  T[3] = 18;
	  T[4] = 17;
	  T[5] = 2;
	  T[6] = 14;
	  T[7] = 9;
	  T[8] = 3;
	  T[9] = 1;

	  printTableau(T, 10);

	  quicksort(T, 0, 9);

	  printTableau(T, 10);

	  return 0;
  }

  void swap(int *op1, int *op2)
  {
	  int temp = *op1;
	  ,*op1 = *op2;
	  ,*op2 = temp;
  }

  int rearrangement(int T[], int p, int r)
  {
	  int v = T[r];
	  int i = p;

	  for (int j = p; j < r; ++j) {
		  if (T[j] <= v) {
			  swap(T + i, T + j);
			  ++i;
		  }
	  }

	  swap(T + i, T + r);

	  return i;
  }

  void quicksort(int T[], int p, int r)
  {
	  if (p < r) {
		  int q = rearrangement(T, p, r);
		  quicksort(T, p, q - 1);
		  quicksort(T, q + 1, r);
	  }
  }

  void printTableau(int T[], int size)
  {
	  for (int i = 0; i < size; ++i) {
		  printf("%d ", T[i]);
	  }

	  printf("\n");
  }

#+END_SRC


*** Preuve de terminaison




* Cours 1 : 17/09/2019 et 24/09/2019

** Chapitre 0

*** Notion de complexité

#+BEGIN_DEFINITION
Soit n la taille de l'entrée, et k une constante.

P : Se dit des problèmes qui se calculent en temps polynomial O(n^k)
EXP : Se calculent en temps exponentiel O(2^n)
NP : intermédiaire (Est-ce que les problèmes intermédiaires sont des problèmes P ou des problèmes NP-difficiles)
#+END_DEFINITION

On ne parlera pas des problèmes exponentiels.

Exemples de problèmes polynômiaux : tri, recherche, géométrie, texte, arithmétique.

*** Analyse d'algorithmes

Il faut définir une notion de taille (pas univoque, on peut en définir plusieurs).

Pour donner une complexité (nlog(n)), il faut donner aussi l'opération effectuée (permutation, etc...)

L'opération fondamentale doit être explicitée pour pouvoir permettre les comparaisons.

Plusieurs choses peuvent être comparées :
- Dans le meilleur des cas : min{ T_A(e) ; e \in E_n}
- Dans le pire des cas : max{ T_A(e) ; e \in E_n}
- En moyenne : 1 / (|E_n|) * \sum_{e \in E_n} T_A(e)

#+BEGIN_QUOTE
On pourrait à la limite donner une distribution de probabilité (et pas seulement un moment) d'un algorithme.
#+END_QUOTE

On introduit une nouvelle notion : complexité amortie, définie comme le coût d'une suite d'opération (donc moyenne des coûts).

*** Notions de mathématiques

On définit les trois notions principales :

#+BEGIN_DEFINITION
Soit $\mathcal{F}_{\mathcal{N}}$ l'ensemble des fonctions de $\mathbb{N}$ dans $\mathbb{R}^{+}$.

Soient $f$ et $g$ deux fonctions de $\mathcal{F}_{\mathcal{N}}$.

On dit que $f$ est dominée par $g$ au voisinage de l'infini (ou encore que $g$ est une borne asymptotique supérieure de $f$) sssi :

$\exists c, n_0 > 0$ tels que $\forall n > n_0$, on a $f(n) < cg(n)$.

On peut noter $f = O(g)$ ou encore $g = \Omega(f)$, ces deux notations sont équivalentes.
#+END_DEFINITION

#+BEGIN_DEFINITION
On dira que $f$ et $g$ sont semblables au voisinage de l'infini (ou encore que $g$ est une borne asymptotique approchée de $f$) sssi $f$ est dominée par $g$ et $g$ est dominée par $f$. On notera :

$f = \Theta(g)$ (et donc aussi $g = \Theta(f)$)
#+END_DEFINITION

#+BEGIN_DEFINITION
On a aussi la notation suivante :

$\lim_{n \to +\infty} f(n)/g(n) = 0$ se note $f = o(g)$

De fait, $f = o(g)$ implique $f = O(g)$

$f ~ g$ signifie $f - g = o(g)$ (équivalence)
#+END_DEFINITION

[[./CM1/ordredegrandeur.jpg][Comparaison d'ordres de grandeur asymptotique]]


** Chapitre 1 : Files de priorité

Complexité amortie : On est au plus proche de ce qui se passe en pratique.

Interclassement de liste : linéaire en la somme des tailles des deux listes.

*** Opérations de files de priorité

Ensemble d'éléments, chaque d'élément identifié par une clé, on veut trouver le minimum des clés. (typiquement une valeur de priorité pour un ordonnanceur)

Il faut un ordre total : on doit pouvoir comparer toujours deux éléments : on doit pouvoir dire cet élément-ci est plus petit/égal/plus grand que celui-là.


Opérations :

- On veut pouvoir ajouter un élément
- Supprimer l'élément de plus petite clé
- Construire une file avec n éléments reçus à la volée
- Union de plusieurs files de priorités
- Modification d'une clé

*** Tas

Un tas minimum : [insérer image]

#+ATTR_ORG: :width 600
[[./CM1/tasminimum.jpg][Tas minimum]]

Ensemble de valeurs distinctes deux à deux sous la forme d'un arbre. Contrainte : si on part de la racine vers les feuilles, tous les chemins possibles sont des suites strictement croissantes.

Trier un tas minimum est non-trivial (pas en temps linéaire).

On peut le construire en temps n, *donc* on le trie au moins en temps nlog(n)

*** Représentations des données et efficacité

#+ATTR_ORG: :width 600
[[./CM1/representationefficacite.jpg][Représentation et efficacité]]

[expliquer algorithme du tas, insertion]

*** Exemples

- Tri par tas (au lieu d'une liste) (heapsort)
- Sur les graphes (plus court chemin : Dijkstra ou A*), (plus court chemin entre tous les couples de sommets : Johnson), (arbre couvrant minimal : Prim)
- Interclassement de listes triées
- Compression de Huffmann


** Arbre binomial

Un arbre binomial est un graphe avec une racine et des sous-branches qui sont aussi des arbres binomiaux.

N'existent que si la taille est une puissance de 2.

#+BEGIN_DEFINITION
Définition par récurrence :

- B_0 est l'arbre de taille 2^0, un seul noeud

- Étant donné 2 arbres binomiaux B_k, on obtient B_{k+1} en faisant de l'un des B_k le premier fils (à gauche donc) à la racine de l'autre B_k.
#+END_DEFINITION

#+BEGIN_DEFINITION
Définition par induction forte :

- B_0 est l'arbre de taille 2^0, un seul noeud

- Pour construire B_k, je construis une nouvelle racine et je place tous les B_{k-1}, B_{k-2}, ..., B_0 de gauche à droite.
#+END_DEFINITION

Structure dite plane : les fils sont ordonnés de gauche à droite.

#+BEGIN_THEOREM
Un certain nombre de résultats intéressants :

Pour k >= 0

- B_k a 2^k noeuds (par construction)
- B_k a 2^k - 1 arêtes (suit de la précédente)
- B_k a hauteur k (démontrable par récurrence) (hauteur = nombre d'arêtes à traverser depuis la racine vers le fils le plus à gauche)
- Le degré à la racine (arité : nombre de fils) est k.
- Le nombre de noeuds à profondeur i est i parmi k.
- La forêt reliée à la racine de B_k est < B_{k-1}, B_{k-2}, ..., B_1, B_0 > (par construction)
#+END_THEOREM

#+BEGIN_PROOF
- Le premier résultat est évident par construction.
- Le deuxième suit du précédent trivialement : tous les noeuds ont exactement une arête entrante (qui vient du haut), sauf le noeud racine.
- Le troisième résultat se démontre par récurrence faible :

Initialisation (à 1) : évident.
Récurrence : Supposons la chose vraie à n un rang fixe. La construction de B_{n+1} supposant de placer B_n, B_{n-1}, etc... sous une nouvelle racine, dans l'ordre donné, de gauche à droite. Par définition de la hauteur, le chemin à parcourir est l'arête du nouvel arbre jusqu'à son fils immédiat le plus à gauche (1 traversée), puis de ce fils (ancienne racine de l'arbre B_n) jusqu'à son descendant final le plus à gauche (n traversées, par hypothèse de récurrence). On a donc bien n+1 traversées à faire pour arriver au descendant final le plus à gauche. Par application du principe de récurrence, on a B_k a hauteur k. CQFD

- Le quatrième résultat suit de la définition par induction forte plus haut, et suit directement du sixième résultat (qu'on suppose lui vrai par construction) : si la forêt reliée à la racine de B_k est <B_{k-1}, B_{k-2}, ...., B_{1}, B_{0}>

- Le cinquième résultat est le moins trivial à démontrer :

Par construction (en prenant la définition par induction forte), le nombre de noeuds à profondeur i de B_{k+1} est égal au nombre de noeuds à profondeur i de B_k (l'arbre B_k qu'on a fait le père de l'autre) plus le nombre de noeuds à profondeur i-1 de B_k (l'arbre B_k qu'on a fait le fils le plus à gauche de l'autre).

Si on donne n_{k,i} le nombre de noeuds de profondeur i de l'arbre B_k, alors la traduction formelle de la précédente intuition est donnée par :
$n_{k + 1,i} = n_{k,i} + n_{k,i-1}$

A partir de ça, on peut démontrer ce résultat par récurrence.
Initialisation (à 1) : ${1\choose 0} = {1\choose 1} = 1$. On a bien un noeud de profondeur 0 et un de profondeur 1.
On suppose l'égalité vérifiée au rang n, soit $n_{k,i} = {k\choose i}$. On doit montrer $n_{k+1,i} = {k+1\choose i}$, ce qui revient à montrer, par application de la formule de Pascal $n_{k+1,i} = {k\choose i} + {k\choose i-1}$.

Or, par application de la précédente intuition, on voit immédiatement le résultat à démontrer.
#+END_PROOF

*** Aparté

#+BEGIN_DEFINITION
Induction faible vs induction forte

Induction faible : je suppose P_k vraie
Induction forte : je suppose P_k, P_{k-1}, ..., P_0 vraies
#+END_DEFINITION


** File binomiale

#+BEGIN_DEFINITION
Un tournoi binomial (ou tas binomial) est un arbre binomial étiqueté croissant (croissance stricte sur tout chemin de la racine aux feuilles).

En plus, on veut que toutes les étiquettes soient distinctes.
#+END_DEFINITION

#+BEGIN_DEFINITION
Une file binomiale est une suite de tournois binomiaux de tailles strictement décroissantes.
#+END_DEFINITION

#+BEGIN_THEOREM
Corollaire de cette définition :

On peut donner une file binomiale (et une seule !) pour écrire n'importe quel nombre entier d'étiquettes. 

Quand on dit une seule, c'est la structure qui est unique, pas la manière d'étiqueter la structure.
#+END_THEOREM

#+BEGIN_THEOREM
Extension du théorème démontré en archi :

n = \alpha_0 2^0 + \alpha_1 2^1 + ... + \alpha_r 2^r

*Avec r = floor(log_2(n))*

log_2 est le logarithme binaire, soit la puissance à laquelle le nombre 2 doit être monté pour obtenir un nombre n.
#+END_THEOREM

#+BEGIN_THEOREM
Corollaire

Au plus, on a log(n) éléments dans la file. On sait que le minimum de toutes les étiquettes est à la racine d'un des éléments de la file. Donc la complexité de trouver le min dans une file binomiale est au plus d'ordre log(n).
#+END_THEOREM

#+BEGIN_THEOREM
Corollaire

Si n = 2^k, FB_n est la suite réduite à un unique tournoi binomial, le tournoi binomial TB_k.

Sinon la file binomiale est une suite de tournois correspondants aux bits égaux à 1 dans la représentation binaire de n.
#+END_THEOREM

#+BEGIN_DEFINITION
Poids de Hamming :

\nu(n) = \sum_i b_i

avec les b_i les bits à 1 dans la représentation binaire de n. Correspond au nombre de tournois qu'il faut pour écrire n.
#+END_DEFINITION

#+BEGIN_THEOREM
Propriétés de FB_n

- FB_n a n noeuds
- FB_n a (n - \nu(n)) arêtes.
- Le plus grand arbre binomial est B_{floor(log(n))}, le premier arbre de la file binomiale. De hauteur floor(log(n)) et nombre de noeuds 2^{floor(log(n))}.
- Le nombre d'arbres de la file est donné par \nu(n).
- Le minimum de la file est à la racine de l'un des arbres
#+END_THEOREM

#+BEGIN_PROOF
Le premier résultat est la définition.

Le deuxième résultat vient de ce que le nombre d'arêtes correspond au nombre de noeuds, moins le nombre de tournois (extension de la preuve plus haut par arête entrante, à \nu(n) plutôt que 1).

Le troisième résultat est vrai par construction.

Le quatrième est donné par la définition du poids de Hamming.

Le cinquième suit des conditions de stricte croissance et stricte distinction des étiquettes de chacun des tournois de la file.
#+END_PROOF

*** Union de files binomiales

On suppose que toutes les clés sont distinctes dans les N files à unir.

- Cas 1, union de 2 tournois (TB_l, TB_k) de tailles différentes :

D'après notre supposition, le cardinal de l'ensemble des clés des 2 tournois est de 2^l + 2^k.

On peut simplement donner F = < TB_l, TB_k >, qui est une file binomiale valable.

- Cas 2, union de 2 tournois de même taille.

On peut faire une file binomiale : F = TB_{k+1} qui pourra contenir toutes les étiquettes des deux tournois en entrée. Cette file binomiale doit respecter la propriété selon laquelle la racine est le plus petit élément du tournoi : on prend en fils de l'autre celui qui a la plus grande racine.

- Union de 2 files binomiales correspond à une addition binaire.

<TB_2, TB_0> U <TB'_2, TB'_1, TB'_0> = <TB''_3, TB''_2>

**** Aparté : l'addition binaire

En fait, c'est super facile de faire une addition en binaire : on peut se contenter de faire du bit par bit.

1 + 0 = 0 + 1 = 1
0 + 0 = 0
1 + 1 = +1 au bit de gauche (au bit de poids plus fort) (et 0 au bit courant)

**** Primitives

Ces primitives des pages 22 et 23 sont utilisables en examen : si on les appelle dans du pseudo-code, le correcteur (notre compilateur humain) saura ce que ça signifie.

On a droit à :

- EstVide(T) : Renvoie vrai si le tournoi T est vide
- Degre(T) : Renvoie le degré (un entier) de la racine du tournoi
- Union2Tid(T) : Renvoie l'union de deux tournois (un tournoi) de même taille
- Decapite(T) : Renvoie la file binomiale (suite de tournois) obtenue en supprimant la racine du tournoi T_k
- File(T) : Renvoie la file binomiale réduite au tournoi

- EstVide(F) : Renvoie vrai si la file F est vide
- MinDeb(F) : Renvoie le tournoi de degré minimal de la file F
- Reste(F) : Renvoie la file privee de son tournoi de degre minimal
- AjoutMin(T, F) : Renvoie la file obtenue en ajoutant le tournoi T comme tournoi de degré inférieur de la file F (ne fonctionne que si T est effectivement de degré plus petit que le degré minimal de la file passée en entrée)

Ces primitives peuvent aussi servir à définir des algorithmes de plus haut niveau. Si on a le temps, on pourrait implémenter ces primitives en C.

**** Analyse de complexité

On ne prouve pas la correction de l'algorithme (aussi vrai, ni plus ni moins, que la somme binaire).

La complexité de l'union de FB_n et FB_m est en O(log_2(n+m))

Critère de complexité : nombre de comparaisons entre clés (la création de pointeurs, copies de données ne comptent pas).

Idée principale : L'union de deux tournois de même taille nécessite 1 comparaison entre clés et ajoute une arête dans le file résultat. (l'union de deux tournois de taille différente est triviale : tournoi+grand, tournoi-grand, et ne nécessite pas de création d'arête ni de comparaion)

Conséquence : Le nombre de comparaisons égale le nombre d'arêtes de la file union moins le nombre d'arêtes des files de départ (en gros, les arêtes créées).

#+BEGIN_PROOF
Si on pose cette idée de manière formelle, ça donne :

$n + m - \nu(n+m) - (n - \nu(n)) - (m - \nu(m))$

Ce qui va se simplifier en :
$\nu(n) + \nu(m) - \nu(n+m)$

On a \nu(n) toujours inférieur à ceiling(log_2(n)).

On peut donc poser la quantité plus haut comme inférieure largement à :

$2\lfloor log_2(n + m) \rfloor + 2$

Cette borne supérieure est un grand O de log_2(n + m)
#+END_PROOF

*** Ajout d'un élément à une file

La meilleure manière, c'est de créer une file binomiale contenant seulement l'élément à ajouter, puis je fais l'union.

La complexité est donc donnée par \nu(n) + 1 - \nu(n + 1).

La complexité est entre 0 et \nu(n) : on a au max \nu(n) comparaisons.

*** Construction

La meilleure manière, c'est l'ajout de chacun des éléments un à un.

La complexité de la construction d'une file binomiale est donc donnée par la complexité de l'adjonction successive de ses n éléments.

Soit ici :

$\sum_{i=1}^{n-1} [\nu(i) + i - \nu(1+i)]$

C'est une somme téléscopique. Après simplification, la quantité totale de comparaisons est donnée par n - \nu(n)

Coût de la construction est le coût de la commutation de bits. [à clarifier avec Genetrini : Qu'est-ce que la commutation de bits ?]

**** Aparté : le coût amorti

#+BEGIN_DEFINITION
Dans une série d'opérations, on peut définir le coût amorti d'une opération comme la division du coût total par le nombre des opérations.

La définition plus générale, c'est le coût moyen d'une opération au pire cas. La manière de la définir plus haut est le résultat de la méthode dite par agrégat.
#+END_DEFINITION

*** Suppression du minimum

On sait par construction que le minimum de la file est à la racine d'un des tournois qui la composent.

La recherche prend donc \nu(n) - 1 comparaisons (le nombre de tournois - 1). Ce qui est un grand O de log_2(n).

La suppression consiste en supprimer la racine du tournoi qui porte la racine minimale : ça créé une deuxième file binomiale constituée des fils orphelins de la racine. On fait ensuite l'union de cette nouvelle file avec la file de départ privée du tournoi décapité.

Il reste, entre ces deux files, n - 1 éléments.

La recherche est toujours de complexité log_2(n). L'union des deux files sera aussi de complexité log_2(n) dans le pire des cas (cas particulier de la complexité général des unions de files binomiales).

*** Diminution d'une clé (ou étiquette)

On suppose un accès direct au noeud dont il faut diminuer la clé.

La meilleure manière, c'est de modifier la clé, puis échanger le noeud avec son père de manière successive jusqu'à ce que l'hypothèse de stricte croissance soit à nouveau respectée. Par construction, il y a forcément un moment où ça arrive. Par construction, on suppose aussi que la nouvelle valeur de la clé est distincte de toutes les autres (hypothèse de stricte distinction).

La complexité au pire cas, donné par le nombre maximum de comparaisons, est donc la hauteur de l'arbre, soit log_2(n) (ou log(n), à vérifier avec Genitrini)

* Annexes





